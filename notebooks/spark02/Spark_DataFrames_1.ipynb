{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_DataFrames_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqFkDzNCs58440KWKzDxMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasborrella/TheValley/blob/main/notebooks/spark02/Spark_DataFrames_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeQLYo17pIcm"
      },
      "source": [
        "# Spark DataFrames\n",
        "\n",
        "Notebook por [Tomás Borrella Martín](https://www.linkedin.com/in/tomasborrella/).\n",
        "\n",
        "### Enlaces de interés\n",
        "*   [Slides de presentación](https://docs.google.com/presentation/d/1MotclVSrLoykWogG-WwLa-DbPNvVgHBaGuZJX2Gfc4o/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnun5POxh_hX"
      },
      "source": [
        "# 1. Instalación Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuJjskSn9KeV"
      },
      "source": [
        "# Install JAVA\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oTR3cFa9ORq"
      },
      "source": [
        "# Install Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPnAX8i9XDc"
      },
      "source": [
        "# Install findspark\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4QagUl9fy7"
      },
      "source": [
        "# Environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sov8DPmr9nhE"
      },
      "source": [
        "# Find spark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g4Frp0P9q5c"
      },
      "source": [
        "# PySpark \n",
        "!pip install pyspark==3.1.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQZYF3RieR1"
      },
      "source": [
        "# 2. Spark Session\n",
        "Punto de entrada de la aplicación de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haPHnvwf9-Mz"
      },
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhcSzKETDLL"
      },
      "source": [
        "# Create Spark Session\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"Spark Dataframes\")\n",
        "         .getOrCreate()\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRxzlGwr-JGl"
      },
      "source": [
        "# Show config\n",
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1akpXVgirGqu"
      },
      "source": [
        "# Cargar datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shskq4JD9bLd"
      },
      "source": [
        "# Descargamos los datos desde Github al entorno de Colab\n",
        "!wget -P /content/data 'https://raw.githubusercontent.com/tomasborrella/TheValley/main/data/spark02/songs_log_small.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hkASbMOAvfF"
      },
      "source": [
        "Leemos el archivo con `SparkSession.read`\n",
        "\n",
        "Al leer un archivo externo con Spark crea un DataFrame\n",
        "\n",
        "[Documentación oficial de read](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.read.html#pyspark.sql.SparkSession.read)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qPBnboS9iVD"
      },
      "source": [
        "# Cargamos los datos en Spark\n",
        "path = '/content/data/songs_log_small.json'\n",
        "user_log = spark.read.json(path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rdQ0cTlF2Hb"
      },
      "source": [
        "# Explorar datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwO2lARmAphh"
      },
      "source": [
        "Comprobamos de qué tipo es el objeto que ha creado (DataFrame):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3rkvza9-CDx"
      },
      "source": [
        "type(user_log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcjQrkeSBDa9"
      },
      "source": [
        "Mostramos información de su esquema con printSchema\n",
        "\n",
        "[Documentación oficial de printSchema](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html#pyspark.sql.DataFrame.printSchema)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwOaqtYP-HdX"
      },
      "source": [
        "user_log.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8oE5g-1BO3z"
      },
      "source": [
        "El método `describe` calcula estadísticas básicas de las columnas. Es una transformación y por lo tanto es *lazy*\n",
        "\n",
        "[Documentación oficial de describe](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.describe.html#pyspark.sql.DataFrame.describe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fKRfpA0-Uus"
      },
      "source": [
        "user_log.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUznGVvKCgJc"
      },
      "source": [
        "Es necesesario utilizar la acción `show` para que realice los cálculos y muestre el resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5qO4casFcet"
      },
      "source": [
        "user_log.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EhSFzQBDBtX"
      },
      "source": [
        "La acción `show` muestra por defecto las primeras 20 líneas del dataframe, pero admite el número de líneas como parámetro\n",
        "\n",
        "[Documentación oficial de show](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.show.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ue-bD-l-f1d"
      },
      "source": [
        "user_log.show(n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62vpOFt5CvAE"
      },
      "source": [
        "Se puede usar describe sobre una sola columna (o un subconjunto de ellas), para \"seleccionar\" la/s columna/s se utiliza `select`\n",
        "\n",
        "[Documentación oficial de select](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcS1xIN8FgvM"
      },
      "source": [
        "user_log.select('sessionId', 'ts').describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir2KOGB5EGCn"
      },
      "source": [
        "`select` es una transformación, y por lo tanto es *lazy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpxSMxmEAD0"
      },
      "source": [
        "user_log.select('sessionId')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlmA42L8E6Kv"
      },
      "source": [
        "Como hemos visto previamente si queremos mostrar el contenido (20 primeras líneas) utilizaríamos show:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KFvRXtZEzrs"
      },
      "source": [
        "user_log.select('sessionId').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iip6qNlqFZGS"
      },
      "source": [
        "Si queremos obtener todos los registros como una lista de filas utilizamos la acción `collect`\n",
        "\n",
        "[Documentación oficial de collect](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.collect.html#pyspark.sql.DataFrame.collect)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuv-cghVFUZd"
      },
      "source": [
        "user_log.select('sessionId').collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH9gmiXnGUsq"
      },
      "source": [
        "Si el resultado lo queremos como una lista de Python podemos usar *List Comprehension*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8T3j1BHGKsQ"
      },
      "source": [
        "mi_lista = [row[0] for row in user_log.select('sessionId').collect()]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy0RwNmSGiJa"
      },
      "source": [
        "type(mi_lista)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y29Bl8A0GjsG"
      },
      "source": [
        "mi_lista"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJefaBzFGw7z"
      },
      "source": [
        "Si en lugar de todos los registros como una lista quisiéramos solo una parte de ellos podemos usar take\n",
        "\n",
        "[Documentación oficial de take](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.take.html#pyspark.sql.DataFrame.take)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDxjOIeS-tAt"
      },
      "source": [
        "user_log.select('sessionId').take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29_i-G4HN26"
      },
      "source": [
        "Para contar el número de registros de un DataFrame se utiliza `count`\n",
        "\n",
        "[Documentación oficial de count](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.count.html#pyspark.sql.DataFrame.count)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E29UakKAHiJ"
      },
      "source": [
        "user_log.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJO-CIecGBD8"
      },
      "source": [
        "# Guardar datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGM3JO4xICxX"
      },
      "source": [
        "Para escribir datos se utiliza `write`\n",
        "\n",
        "[Documentación oficial de write](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.write.html#pyspark.sql.DataFrame.write)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMAjaV1x-9XK"
      },
      "source": [
        "user_log.write.save('data/songs_log_small.csv', format='csv', header=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfvkiRwMIthD"
      },
      "source": [
        "Después de escribirlos volvemos a leerlos para comprobarlos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWk2C_58_sP6"
      },
      "source": [
        "user_log_2 = spark.read.csv('data/songs_log_small.csv', header=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C83VIzD_1-8"
      },
      "source": [
        "user_log_2.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No4pu93W_4KU"
      },
      "source": [
        "user_log_2.take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B9m6R6U_8iR"
      },
      "source": [
        "user_log_2.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDf1fI6AKhP"
      },
      "source": [
        "user_log_2.select(\"userID\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5iMY3OtAQ8Y"
      },
      "source": [
        "user_log_2.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ldBck9RGMpn"
      },
      "source": [
        "Si comprobamos en el sistema de archivos podemos ver que tenemos tantos archivos como particiones tuviéramos (en este caso 2).\n",
        "\n",
        "Si quisiéramos tener un único archivo, tendríamos que reparticionar los datos usando `coallesce(1)`\n",
        "\n",
        "[Documentación oficial coallesce](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html#pyspark.sql.DataFrame.coalesce)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjEbO9pXFN8e"
      },
      "source": [
        "user_log.coalesce(1).write.save('data/songs_log_small_one.csv', format='csv', header=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XSc9RBqsQgb"
      },
      "source": [
        "# Spark Stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHm2F98TkLx"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}