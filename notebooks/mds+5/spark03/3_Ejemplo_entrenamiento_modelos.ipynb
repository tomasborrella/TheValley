{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SpQZYF3RieR1"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPV+n4xXqxlUFO8q1SAt39S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasborrella/TheValley/blob/main/notebooks/mds%2B5/spark03/3_Ejemplo_entrenamiento_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeQLYo17pIcm"
      },
      "source": [
        "# Ejemplo de entrenamiento de modelos\n",
        "\n",
        "Notebook por [Tomás Borrella Martín](https://www.linkedin.com/in/tomasborrella/)\n",
        ".\n",
        "\n",
        "Usando los datos de salarios de [este dataset](https://archive.ics.uci.edu/ml/datasets/Adult), predecir si el salario es mayor o menor de 50K$ utilizando los datos censales.\n",
        "\n",
        "### Enlaces de interés\n",
        "*   [Slides de presentación](https://docs.google.com/presentation/d/176cobMzuzy_mRRe3YumHBoap98PvFNXhhgIxN1mYmH0/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnun5POxh_hX"
      },
      "source": [
        "# 1. Instalación Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuJjskSn9KeV"
      },
      "source": [
        "# Install JAVA\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oTR3cFa9ORq"
      },
      "source": [
        "# Install Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPnAX8i9XDc"
      },
      "source": [
        "# Install findspark\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4QagUl9fy7"
      },
      "source": [
        "# Environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sov8DPmr9nhE"
      },
      "source": [
        "# Find spark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g4Frp0P9q5c"
      },
      "source": [
        "# PySpark \n",
        "!pip install pyspark==3.2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQZYF3RieR1"
      },
      "source": [
        "# 2. Spark Session\n",
        "Punto de entrada de la aplicación de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haPHnvwf9-Mz"
      },
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhcSzKETDLL"
      },
      "source": [
        "# Create Spark Session\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"Spark Dataframes\")\n",
        "         .getOrCreate()\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrLc1SbZqkE_"
      },
      "source": [
        "# Ejemplo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M82IhtltFaQc"
      },
      "source": [
        "# Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shskq4JD9bLd"
      },
      "source": [
        "# Descargamos los datos al entorno de Colab\n",
        "!wget -P /content/data 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rdQ0cTlF2Hb"
      },
      "source": [
        "Nos hacemos una primera idea de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYYlpU1Azh7I"
      },
      "source": [
        "!head /content/data/adult.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ymAcQez2bm"
      },
      "source": [
        "La descripción completa del dataset está en [este enlace](https://archive.ics.uci.edu/ml/datasets/Adult)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4_welA0Dch"
      },
      "source": [
        "Cargamos los datos en un DataFrame especificando el esquema:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IMWpmiPz1NZ"
      },
      "source": [
        "from pyspark.sql.types import DoubleType, StringType, StructField, StructType\n",
        " \n",
        "schema = StructType([\n",
        "  StructField(\"age\", DoubleType(), False),\n",
        "  StructField(\"workclass\", StringType(), False),\n",
        "  StructField(\"fnlwgt\", DoubleType(), False),\n",
        "  StructField(\"education\", StringType(), False),\n",
        "  StructField(\"education_num\", DoubleType(), False),\n",
        "  StructField(\"marital_status\", StringType(), False),\n",
        "  StructField(\"occupation\", StringType(), False),\n",
        "  StructField(\"relationship\", StringType(), False),\n",
        "  StructField(\"race\", StringType(), False),\n",
        "  StructField(\"sex\", StringType(), False),\n",
        "  StructField(\"capital_gain\", DoubleType(), False),\n",
        "  StructField(\"capital_loss\", DoubleType(), False),\n",
        "  StructField(\"hours_per_week\", DoubleType(), False),\n",
        "  StructField(\"native_country\", StringType(), False),\n",
        "  StructField(\"income\", StringType(), False)\n",
        "])\n",
        " \n",
        "dataset = spark.read.format(\"csv\").schema(schema).load(\"/content/data/adult.data\")\n",
        "cols = dataset.columns"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amim3Y5h0OdP"
      },
      "source": [
        "dataset.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nON9l8lE0czI"
      },
      "source": [
        "# Preprocesado de los datos\n",
        "\n",
        "Creamos un Pipeline con todas las transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wD69TpT2Cvx"
      },
      "source": [
        "Para usar algoritmos como la *Logistic Regression*, primero tenemos que convertir las variables categóricas en valores numéricos.\n",
        "\n",
        "En este notebook vamos a usar una combinación de *StringIndexer* (que asigna un valor numérico a cada categoría) y *OneHotEncoder* (que combierte cada categoría en un vector binario).\n",
        "\n",
        "Se crean los stages de todas las variables categóricas usando un bucle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0oOnD_T0zfB"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        " \n",
        "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
        "# variable que va a contenter las stages del Pipeline\n",
        "stages = []\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Primero StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
        "    \n",
        "    # y después OneHotEncoder para convertir variables categóricas en SparseVectors binarios\n",
        "    from pyspark.ml.feature import OneHotEncoder\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    # vamos añadiendo las stages a la variable.\n",
        "    # No se ejecutan ahora, se añadirán al Pipeline más adelante.\n",
        "    stages += [stringIndexer, encoder]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbOGVsQC2s2P"
      },
      "source": [
        "Podemos comprobar que el bucle a través de las 8 variables categóricas ha funcionado bien mirando el contenido de la variable *stages*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-E0E5201Ohn"
      },
      "source": [
        "stages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhv5ORnH22FV"
      },
      "source": [
        "Añadimos también un stage para convertir la variable target (*label*) a numérica usando *StringIndexer*: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NOidPpF21Wl"
      },
      "source": [
        "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
        "stages += [label_stringIdx]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2NM0fJt3aX3"
      },
      "source": [
        "Y por último añadimos un stage de *VectorAssembler* para convertir todas las *features* en un único vector (así es como lo necesitan los modelos de clasificación):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAkoFVFN3waN"
      },
      "source": [
        "# Transformamos todas las features en un vector con VectorAssembler\n",
        "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHIq0rDy4SLg"
      },
      "source": [
        "Ejecutamos todo el Pipeline de preparación y obtenemos un DataFrame que ya estará listo para el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Gyp5RR4Rg-"
      },
      "source": [
        "partialPipeline = Pipeline().setStages(stages)\n",
        "pipelineModel = partialPipeline.fit(dataset)\n",
        "preppedDataDF = pipelineModel.transform(dataset)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oIRJKXl4fBb"
      },
      "source": [
        "Comprobamos el DataFrame preparado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1eXjXKP4iAO"
      },
      "source": [
        "preppedDataDF.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkd-cpR5lZF"
      },
      "source": [
        "Nos quedamos solo con las columnas que nos interesan (las originales y \"label\" y \"features\" que son las 2 que necesitan los modelos):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac5pqP9B5pUZ"
      },
      "source": [
        "selectedcols = [\"label\", \"features\"] + cols\n",
        "dataset = preppedDataDF.select(selectedcols)\n",
        "dataset.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfL0qqVL6BS9"
      },
      "source": [
        "Partimos el DataFrame en train y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOMRZETJ6FLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a798aa00-f2f0-4178-e9bf-0647d73387c9"
      },
      "source": [
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22832\n",
            "9729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waLHazhi7mzd"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFeGISXj7rI0"
      },
      "source": [
        "## Versión inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtJEaYqZ7pZS"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        " \n",
        "# Se crea un modelo inicial\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        " \n",
        "# Se entrena el modelo con los datos de train\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "# Predecimos sobre los datos e test, para ello usamos el método transform().\n",
        "# LogisticRegression.transform() realmente solo necesita la columna 'features'.\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "# Visualizamos la salida del modelo (predicciones y probabilidad de cada clase) \n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "# NOTA: Se podrían haber seleccionado otras columnas adicionales.\n",
        "selected.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPghtRZ8DX5"
      },
      "source": [
        "Para evaluar el modelo podemos usar  *BinaryClassificationEvaluator*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mb8wYl48ARB"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        " \n",
        "# Creamos el evaluador\n",
        "evaluator = BinaryClassificationEvaluator()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Uujx9Q8X8y"
      },
      "source": [
        "La métrica que este evaluador va usar por defecto es el AUC (*Area Under the Curve*), pero podríamos hacer que usara *areaUnderPR* de la siguiente manera:\n",
        "`evaluator.setMetricName(\"areaUnderPR\")`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncF4kf18i0M"
      },
      "source": [
        "evaluator.getMetricName()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_3HXv0Y8kiB"
      },
      "source": [
        "Evaluamos las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6pCecz8fIw"
      },
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFSU2JH9AZs"
      },
      "source": [
        "## Tuning\n",
        "\n",
        "Se va a afinar el modelo usando *ParamGridBuilder* y *CrossValidator*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zMzXnmN9NNl"
      },
      "source": [
        "Para saber qué parámetros podemos modificar de este modelo usamos `explainParams()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u0oKLj49iQW"
      },
      "source": [
        "print(lr.explainParams())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF5lCP239plT"
      },
      "source": [
        "Si usamos tres valores para *regParam*, tres para *maxIter*, y dos para *elasticNetParam*, las combinaciones de parámetros serán 3 x 3 x 3 = 27 posibilidades para el *CrossValidator*.\n",
        "\n",
        "**Esto va a llevar mucho tiempo en una sola máquina**\n",
        "\n",
        "Para las pruebas podemos reducirlo a 2 x 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrcdzm5c974b"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        " \n",
        "# Se crea el ParamGrid que se usará en el CrossValidator\n",
        "# Esta es la versión simplificada para que tarde poco\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.5])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
        "             .build())\n",
        "\n",
        "# Esta sería una versión más completa que tarda demasiado sin un cluster\n",
        "# paramGrid = (ParamGridBuilder()\n",
        "#              .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
        "#              .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "#              .addGrid(lr.maxIter, [1, 5, 10])\n",
        "#              .build())\n",
        "\n",
        "# Se crea un CrossValidator de  5-fold \n",
        "cv = CrossValidator(estimator=lr, \n",
        "                    estimatorParamMaps=paramGrid, \n",
        "                    evaluator=evaluator, \n",
        "                    numFolds=5)\n",
        " \n",
        "# Se ejecuta el CrossValidator (con los 5-folds y el ParamGrid)\n",
        "cvModel = cv.fit(trainingData)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz69JPDz-0DZ"
      },
      "source": [
        "Usamos el nuevo modelo para hacer una predicción sobre los datos de test y medir su precisión:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpHyzLhW_AFl"
      },
      "source": [
        "# Usamos los datos de test para crear una nueva predicción\n",
        "# cvModel utiliza el mejor modelo encontrado en la validación cruzada\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Y evaluamos las predicción\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcmJx3B-_I2u"
      },
      "source": [
        "Podemos ver los pesos de los coeficientes y el intercepto del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nMqr4l_Lor"
      },
      "source": [
        "print('Model Intercept: ', cvModel.bestModel.intercept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZVqCJGw_Wnx"
      },
      "source": [
        "weights = cvModel.bestModel.coefficients\n",
        "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
        "weightsDF = spark.createDataFrame(weights, [\"Feature Weight\"])\n",
        "weightsDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJP9gmLj_k1g"
      },
      "source": [
        "Y por último podemos echar un vistazo a las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y34Tjq8l_oww"
      },
      "source": [
        "# Ver las predicciones del mejor modelo obtenido en la validación cruzada\n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "selected.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rz289wNAR-k"
      },
      "source": [
        "# Ejercicio propuesto: Random Forest\n",
        "\n",
        "Entrenar un *RandomForestClassifier* y comprobar si sus métricas son mejores que las del *LogisticRegression*.\n",
        "\n",
        "1.   Primero una versión inicial del Random Forest\n",
        "2.   Después intentar tuning de hiperparámetros\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XSc9RBqsQgb"
      },
      "source": [
        "# Spark Stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHm2F98TkLx"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}