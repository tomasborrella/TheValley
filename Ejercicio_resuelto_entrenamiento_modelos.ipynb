{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio_resuelto_entrenamiento_modelos.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tnun5POxh_hX",
        "SpQZYF3RieR1"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzhH+W3Nfkla12MKtlMxw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomasborrella/TheValley/blob/main/Ejercicio_resuelto_entrenamiento_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeQLYo17pIcm"
      },
      "source": [
        "# Ejercicio resuelto de entrenamiento de modelos\n",
        "\n",
        "Notebook por [Tomás Borrella Martín](https://www.linkedin.com/in/tomasborrella/)\n",
        ".\n",
        "\n",
        "Usando los datos de salarios de [este dataset](https://archive.ics.uci.edu/ml/datasets/Adult), predecir si el salario es mayor o menor de 50K$ utilizando los datos censales.\n",
        "\n",
        "### Enlaces de interés\n",
        "*   [Slides de presentación](https://docs.google.com/presentation/d/1MotclVSrLoykWogG-WwLa-DbPNvVgHBaGuZJX2Gfc4o/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnun5POxh_hX"
      },
      "source": [
        "# 1. Instalación Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuJjskSn9KeV"
      },
      "source": [
        "# Install JAVA\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oTR3cFa9ORq"
      },
      "source": [
        "# Install Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPnAX8i9XDc"
      },
      "source": [
        "# Install findspark\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4QagUl9fy7"
      },
      "source": [
        "# Environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sov8DPmr9nhE"
      },
      "source": [
        "# Find spark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g4Frp0P9q5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81cec42-0999-4e6c-dae2-16b97dac5330"
      },
      "source": [
        "# PySpark \n",
        "!pip install pyspark==3.1.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 74kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 21.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=fc09b54999d8800bf8338f667256a7e59121f24af006e9a61fe0cf89dc097a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQZYF3RieR1"
      },
      "source": [
        "# 2. Spark Session\n",
        "Punto de entrada de la aplicación de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haPHnvwf9-Mz"
      },
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhcSzKETDLL"
      },
      "source": [
        "# Create Spark Session\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"Spark Dataframes\")\n",
        "         .getOrCreate()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrLc1SbZqkE_"
      },
      "source": [
        "# Ejemplo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M82IhtltFaQc"
      },
      "source": [
        "# Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shskq4JD9bLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1e1956-c4cc-44d8-b6f3-4380266edbdd"
      },
      "source": [
        "# Descargamos los datos al entorno de Colab\n",
        "!wget -P /content/data 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-19 16:29:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3974305 (3.8M) [application/x-httpd-php]\n",
            "Saving to: ‘/content/data/adult.data’\n",
            "\n",
            "adult.data          100%[===================>]   3.79M  3.17MB/s    in 1.2s    \n",
            "\n",
            "2021-06-19 16:29:58 (3.17 MB/s) - ‘/content/data/adult.data’ saved [3974305/3974305]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rdQ0cTlF2Hb"
      },
      "source": [
        "Nos hacemos una primera idea de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYYlpU1Azh7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79574b2d-9f36-483e-98b2-6df03128d3db"
      },
      "source": [
        "!head /content/data/adult.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n",
            "50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n",
            "38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n",
            "53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n",
            "28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n",
            "37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n",
            "49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n",
            "52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n",
            "31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n",
            "42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ymAcQez2bm"
      },
      "source": [
        "La descripción completa del dataset está en [este enlace](https://archive.ics.uci.edu/ml/datasets/Adult)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4_welA0Dch"
      },
      "source": [
        "Cargamos los datos en un DataFrame especificando el esquema:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IMWpmiPz1NZ"
      },
      "source": [
        "from pyspark.sql.types import DoubleType, StringType, StructField, StructType\n",
        " \n",
        "schema = StructType([\n",
        "  StructField(\"age\", DoubleType(), False),\n",
        "  StructField(\"workclass\", StringType(), False),\n",
        "  StructField(\"fnlwgt\", DoubleType(), False),\n",
        "  StructField(\"education\", StringType(), False),\n",
        "  StructField(\"education_num\", DoubleType(), False),\n",
        "  StructField(\"marital_status\", StringType(), False),\n",
        "  StructField(\"occupation\", StringType(), False),\n",
        "  StructField(\"relationship\", StringType(), False),\n",
        "  StructField(\"race\", StringType(), False),\n",
        "  StructField(\"sex\", StringType(), False),\n",
        "  StructField(\"capital_gain\", DoubleType(), False),\n",
        "  StructField(\"capital_loss\", DoubleType(), False),\n",
        "  StructField(\"hours_per_week\", DoubleType(), False),\n",
        "  StructField(\"native_country\", StringType(), False),\n",
        "  StructField(\"income\", StringType(), False)\n",
        "])\n",
        " \n",
        "dataset = spark.read.format(\"csv\").schema(schema).load(\"/content/data/adult.data\")\n",
        "cols = dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amim3Y5h0OdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f76d78-75b5-42aa-82be-cf10767fc782"
      },
      "source": [
        "dataset.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "| age|        workclass|  fnlwgt| education|education_num|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "|39.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
            "|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
            "|38.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
            "|53.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
            "|28.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nON9l8lE0czI"
      },
      "source": [
        "# Preprocesado de los datos\n",
        "\n",
        "Creamos un Pipeline con todas las transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wD69TpT2Cvx"
      },
      "source": [
        "Para usar algoritmos como la *Logistic Regression*, primero tenemos que convertir las variables categóricas en valores numéricos.\n",
        "\n",
        "En este notebook vamos a usar una combinación de *StringIndexer* (que asigna un valor numérico a cada categoría) y *OneHotEncoder* (que combierte cada categoría en un vector binario).\n",
        "\n",
        "Se crean los stages de todas las variables categóricas usando un bucle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0oOnD_T0zfB"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        " \n",
        "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
        "# variable que va a contenter las stages del Pipeline\n",
        "stages = []\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Primero StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
        "    \n",
        "    # y después OneHotEncoder para convertir variables categóricas en SparseVectors binarios\n",
        "    from pyspark.ml.feature import OneHotEncoder\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    # vamos añadiendo las stages a la variable.\n",
        "    # No se ejecutan ahora, se añadirán al Pipeline más adelante.\n",
        "    stages += [stringIndexer, encoder]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbOGVsQC2s2P"
      },
      "source": [
        "Podemos comprobar que el bucle a través de las 8 variables categóricas ha funcionado bien mirando el contenido de la variable *stages*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-E0E5201Ohn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aab4190-2519-4d82-ca9f-3ba5db7c15ef"
      },
      "source": [
        "stages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_d5d764e2c239,\n",
              " OneHotEncoder_bd13a6263131,\n",
              " StringIndexer_2ebc7f3c8fe6,\n",
              " OneHotEncoder_4fe594756cfd,\n",
              " StringIndexer_7822cc7b4e7f,\n",
              " OneHotEncoder_3340bc53f631,\n",
              " StringIndexer_a655ba0fddf0,\n",
              " OneHotEncoder_bf546167c3cc,\n",
              " StringIndexer_755addbd14b5,\n",
              " OneHotEncoder_43360cacf78d,\n",
              " StringIndexer_770cbd967219,\n",
              " OneHotEncoder_160f5a785e8d,\n",
              " StringIndexer_ead56e96175b,\n",
              " OneHotEncoder_060ad367e7bc,\n",
              " StringIndexer_4982e725321c,\n",
              " OneHotEncoder_8f4378c88799]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhv5ORnH22FV"
      },
      "source": [
        "Añadimos también un stage para convertir la variable target (*label*) a numérica usando *StringIndexer*: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NOidPpF21Wl"
      },
      "source": [
        "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
        "stages += [label_stringIdx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2NM0fJt3aX3"
      },
      "source": [
        "Y por último añadimos un stage de *VectorAssembler* para convertir todas las *features* en un único vector (así es como lo necesitan los modelos de clasificación):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAkoFVFN3waN"
      },
      "source": [
        "# Transformamos todas las features en un vector con VectorAssembler\n",
        "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHIq0rDy4SLg"
      },
      "source": [
        "Ejecutamos todo el Pipeline de preparación y obtenemos un DataFrame que ya estará listo para el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Gyp5RR4Rg-"
      },
      "source": [
        "partialPipeline = Pipeline().setStages(stages)\n",
        "pipelineModel = partialPipeline.fit(dataset)\n",
        "preppedDataDF = pipelineModel.transform(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oIRJKXl4fBb"
      },
      "source": [
        "Comprobamos el DataFrame preparado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1eXjXKP4iAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70496dc4-7d72-4a8a-b834-a36e4c1f949d"
      },
      "source": [
        "preppedDataDF.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+--------------+-----------------+--------------+-----------------+-------------------+----------------------+---------------+------------------+-----------------+--------------------+---------+-------------+--------+-------------+-------------------+----------------------+-----+--------------------+\n",
            "| age|        workclass|  fnlwgt| education|education_num|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|workclassIndex|workclassclassVec|educationIndex|educationclassVec|marital_statusIndex|marital_statusclassVec|occupationIndex|occupationclassVec|relationshipIndex|relationshipclassVec|raceIndex| raceclassVec|sexIndex|  sexclassVec|native_countryIndex|native_countryclassVec|label|            features|\n",
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+--------------+-----------------+--------------+-----------------+-------------------+----------------------+---------------+------------------+-----------------+--------------------+---------+-------------+--------+-------------+-------------------+----------------------+-----+--------------------+\n",
            "|39.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|           4.0|    (8,[4],[1.0])|           2.0|   (15,[2],[1.0])|                1.0|         (6,[1],[1.0])|            3.0|    (14,[3],[1.0])|              1.0|       (5,[1],[1.0])|      0.0|(4,[0],[1.0])|     0.0|(1,[0],[1.0])|                0.0|        (41,[0],[1.0])|  0.0|(100,[4,10,24,32,...|\n",
            "|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|           1.0|    (8,[1],[1.0])|           2.0|   (15,[2],[1.0])|                0.0|         (6,[0],[1.0])|            2.0|    (14,[2],[1.0])|              0.0|       (5,[0],[1.0])|      0.0|(4,[0],[1.0])|     0.0|(1,[0],[1.0])|                0.0|        (41,[0],[1.0])|  0.0|(100,[1,10,23,31,...|\n",
            "|38.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|           0.0|    (8,[0],[1.0])|           0.0|   (15,[0],[1.0])|                2.0|         (6,[2],[1.0])|            9.0|    (14,[9],[1.0])|              1.0|       (5,[1],[1.0])|      0.0|(4,[0],[1.0])|     0.0|(1,[0],[1.0])|                0.0|        (41,[0],[1.0])|  0.0|(100,[0,8,25,38,4...|\n",
            "|53.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|           0.0|    (8,[0],[1.0])|           5.0|   (15,[5],[1.0])|                0.0|         (6,[0],[1.0])|            9.0|    (14,[9],[1.0])|              0.0|       (5,[0],[1.0])|      1.0|(4,[1],[1.0])|     0.0|(1,[0],[1.0])|                0.0|        (41,[0],[1.0])|  0.0|(100,[0,13,23,38,...|\n",
            "|28.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|           0.0|    (8,[0],[1.0])|           2.0|   (15,[2],[1.0])|                0.0|         (6,[0],[1.0])|            0.0|    (14,[0],[1.0])|              4.0|       (5,[4],[1.0])|      1.0|(4,[1],[1.0])|     1.0|    (1,[],[])|                9.0|        (41,[9],[1.0])|  0.0|(100,[0,10,23,29,...|\n",
            "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+--------------+-----------------+--------------+-----------------+-------------------+----------------------+---------------+------------------+-----------------+--------------------+---------+-------------+--------+-------------+-------------------+----------------------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkd-cpR5lZF"
      },
      "source": [
        "Nos quedamos solo con las columnas que nos interesan (las originales y \"label\" y \"features\" que son las 2 que necesitan los modelos):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac5pqP9B5pUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1270b0-5ecd-4566-c53a-b87b832761a8"
      },
      "source": [
        "selectedcols = [\"label\", \"features\"] + cols\n",
        "dataset = preppedDataDF.select(selectedcols)\n",
        "dataset.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "|label|            features| age|        workclass|  fnlwgt| education|education_num|     marital_status|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
            "+-----+--------------------+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "|  0.0|(100,[4,10,24,32,...|39.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
            "|  0.0|(100,[1,10,23,31,...|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
            "|  0.0|(100,[0,8,25,38,4...|38.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
            "|  0.0|(100,[0,13,23,38,...|53.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
            "|  0.0|(100,[0,10,23,29,...|28.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
            "+-----+--------------------+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfL0qqVL6BS9"
      },
      "source": [
        "Partimos el DataFrame en train y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOMRZETJ6FLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56075d89-a48e-4380-f178-f3ff5660ad22"
      },
      "source": [
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22832\n",
            "9729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waLHazhi7mzd"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFeGISXj7rI0"
      },
      "source": [
        "## Versión inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtJEaYqZ7pZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d6fac6-12a9-464e-8cc3-e6a5f10826c5"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        " \n",
        "# Se crea un modelo inicial\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        " \n",
        "# Se entrena el modelo con los datos de train\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "# Predecimos sobre los datos e test, para ello usamos el método transform().\n",
        "# LogisticRegression.transform() realmente solo necesita la columna 'features'.\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "# Visualizamos la salida del modelo (predicciones y probabilidad de cada clase) \n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "# NOTA: Se podrían haber seleccionado otras columnas adicionales.\n",
        "selected.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+----+---------------+\n",
            "|label|prediction|         probability| age|     occupation|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "|  0.0|       1.0|[0.16304404160706...|36.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.70118653255393...|32.0| Prof-specialty|\n",
            "|  0.0|       1.0|[0.49801131876699...|33.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.68126165186417...|39.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.61086205071159...|39.0| Prof-specialty|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPghtRZ8DX5"
      },
      "source": [
        "Para evaluar el modelo podemos usar  *BinaryClassificationEvaluator*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mb8wYl48ARB"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        " \n",
        "# Creamos el evaluador\n",
        "evaluator = BinaryClassificationEvaluator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Uujx9Q8X8y"
      },
      "source": [
        "La métrica que este evaluador va usar por defecto es el AUC (*Area Under the Curve*), pero podríamos hacer que usara *areaUnderPR* de la siguiente manera:\n",
        "`evaluator.setMetricName(\"areaUnderPR\")`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncF4kf18i0M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cac0ab7-81ad-402b-ee43-17d754cc3e47"
      },
      "source": [
        "evaluator.getMetricName()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'areaUnderROC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_3HXv0Y8kiB"
      },
      "source": [
        "Evaluamos las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6pCecz8fIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398a146c-73f2-42e6-f82b-7493f0f95749"
      },
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8993574699928725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFSU2JH9AZs"
      },
      "source": [
        "## Tuning\n",
        "\n",
        "Se va a afinar el modelo usando *ParamGridBuilder* y *CrossValidator*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zMzXnmN9NNl"
      },
      "source": [
        "Para saber qué parámetros podemos modificar de este modelo usamos `explainParams()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u0oKLj49iQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bdf6fe-da9d-4d78-91e5-10d0d7b3bc96"
      },
      "source": [
        "print(lr.explainParams())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
            "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
            "featuresCol: features column name. (default: features, current: features)\n",
            "fitIntercept: whether to fit an intercept term. (default: True)\n",
            "labelCol: label column name. (default: label, current: label)\n",
            "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
            "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
            "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0)\n",
            "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
            "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
            "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF5lCP239plT"
      },
      "source": [
        "Si usamos tres valores para *regParam*, tres para *maxIter*, y dos para *elasticNetParam*, las combinaciones de parámetros serán 3 x 3 x 3 = 27 posibilidades para el *CrossValidator*.\n",
        "\n",
        "**Esto va a llevar mucho tiempo en una sola máquina**\n",
        "\n",
        "Para las pruebas podemos reducirlo a 2 x 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrcdzm5c974b"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        " \n",
        "# Se crea el ParamGrid que se usará en el CrossValidator\n",
        "# Esta es la versión simplificada para que tarde poco\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.5])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
        "             .build())\n",
        "\n",
        "# Esta sería una versión más completa que tarda demasiado sin un cluster\n",
        "# paramGrid = (ParamGridBuilder()\n",
        "#              .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
        "#              .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "#              .addGrid(lr.maxIter, [1, 5, 10])\n",
        "#              .build())\n",
        "\n",
        "# Se crea un CrossValidator de  5-fold \n",
        "cv = CrossValidator(estimator=lr, \n",
        "                    estimatorParamMaps=paramGrid, \n",
        "                    evaluator=evaluator, \n",
        "                    numFolds=5)\n",
        " \n",
        "# Se ejecuta el CrossValidator (con los 5-folds y el ParamGrid)\n",
        "cvModel = cv.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz69JPDz-0DZ"
      },
      "source": [
        "Usamos el nuevo modelo para hacer una predicción sobre los datos de test y medir su precisión:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpHyzLhW_AFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c5937d-7c74-4ee4-c843-95db1140d728"
      },
      "source": [
        "# Usamos los datos de test para crear una nueva predicción\n",
        "# cvModel utiliza el mejor modelo encontrado en la validación cruzada\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Y evaluamos las predicción\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8977643264031809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcmJx3B-_I2u"
      },
      "source": [
        "Podemos ver los pesos de los coeficientes y el intercepto del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nMqr4l_Lor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469306c4-bfec-4888-ec00-30d8c3f21e64"
      },
      "source": [
        "print('Model Intercept: ', cvModel.bestModel.intercept)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Intercept:  -1.3832039720849316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZVqCJGw_Wnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32111c4-2b79-47a3-8730-22786ae93820"
      },
      "source": [
        "weights = cvModel.bestModel.coefficients\n",
        "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
        "weightsDF = spark.createDataFrame(weights, [\"Feature Weight\"])\n",
        "weightsDF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|      Feature Weight|\n",
            "+--------------------+\n",
            "|  -0.281603275704831|\n",
            "| -0.6264483359096494|\n",
            "| -0.4360275569860984|\n",
            "| -0.5064247711709583|\n",
            "|  -0.506326689118052|\n",
            "|-0.00494814417175...|\n",
            "| 0.07086989623963032|\n",
            "|   -2.66978938102928|\n",
            "| -0.5593567014148134|\n",
            "|-0.22394378958134853|\n",
            "|  0.5737091727046981|\n",
            "|  0.8976634297736545|\n",
            "|-0.02732965272147547|\n",
            "| -1.2761244527152253|\n",
            "|-0.04222024367536...|\n",
            "| -1.2432064202100166|\n",
            "| -1.7513331537893073|\n",
            "|   1.269765487533909|\n",
            "| -1.4918049429191638|\n",
            "| -0.7975316495227874|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJP9gmLj_k1g"
      },
      "source": [
        "Y por último podemos echar un vistazo a las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y34Tjq8l_oww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d638b7b-7401-41fb-8d1b-82c93467092b"
      },
      "source": [
        "# Ver las predicciones del mejor modelo obtenido en la validación cruzada\n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "selected.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+----+---------------+\n",
            "|label|prediction|         probability| age|     occupation|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "|  0.0|       1.0|[0.23296419268391...|36.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.65520667452462...|32.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.53910224525061...|33.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63734169446424...|39.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.60160343580997...|39.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.59477679040631...|50.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.58993767395651...|51.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.59763736817137...|60.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.69074556096520...|34.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.95784002797462...|20.0| Prof-specialty|\n",
            "|  0.0|       1.0|[0.46810434624054...|35.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.52250674393838...|42.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.55520732570780...|43.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.67103007234566...|48.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.61860385297816...|50.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.70196378588304...|27.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.63220538451931...|32.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.54986701068046...|45.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.60403514252073...|46.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.51936490698105...|56.0|   Craft-repair|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rz289wNAR-k"
      },
      "source": [
        "# Ejercicio propuesto: Random Forest\n",
        "\n",
        "Entrenar un *RandomForestClassifier* y comprobar si sus métricas son mejores que las del *LogisticRegression*.\n",
        "\n",
        "1.   Primero una versión inicial del Random Forest\n",
        "2.   Después intentar tuning de hiperparámetros\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA40jtscG3O2"
      },
      "source": [
        "## Versión inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h03E6i6_G7iJ"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        " \n",
        "# Se crea un modelo de RandomForest inicial.\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        " \n",
        "# Y se entrena con los datos de train\n",
        "rfModel = rf.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SaqrvBUHJxe"
      },
      "source": [
        "# Se realizan las predicciones con el método .transform()\n",
        "predictions = rfModel.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5n-_TONHSvm",
        "outputId": "c4f88558-a876-465b-b955-1b3c2f11629b"
      },
      "source": [
        "# Se muestran las predicciones para hacernos una idea\n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "selected.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+----+---------------+\n",
            "|label|prediction|         probability| age|     occupation|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "|  0.0|       1.0|[0.46234636822485...|36.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|32.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.62414586540324...|33.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|39.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.61375922551731...|39.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|50.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|51.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|60.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63502656323154...|34.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.73466620709577...|20.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63221969275743...|35.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63221969275743...|42.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63221969275743...|43.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63221969275743...|48.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.63221969275743...|50.0| Prof-specialty|\n",
            "|  0.0|       0.0|[0.71319425737735...|27.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.68632500012860...|32.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.68632500012860...|45.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.68632500012860...|46.0|   Craft-repair|\n",
            "|  0.0|       0.0|[0.68632500012860...|56.0|   Craft-repair|\n",
            "+-----+----------+--------------------+----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By08WiTuHbfk"
      },
      "source": [
        "Se evalúa el Random Forest model usando un BinaryClassificationEvaluator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFvbk_gyHjPj",
        "outputId": "a11a547d-e3d8-410c-8c32-0b71757556fb"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        " \n",
        "# Evaluación del modelo\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8853737552309299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64XvDxWWG5QC"
      },
      "source": [
        "## Tuning\n",
        "\n",
        "Vamos a hacer tuning del modelo con *ParamGridBuilder* y *CrossValidator*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM4zlsgVHyYw"
      },
      "source": [
        "Tres valores para *maxDepth*, dos valores para *maxBin*, y dos valores para *numTrees*. El grid de parámetros tiene 3 x 2 x 2 = 12 combinaciones de parámetros para el *CrossValidator*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vyWW8I2Hoqn"
      },
      "source": [
        "# Se crea el ParamGrid\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        " \n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
        "             .addGrid(rf.maxBins, [20, 60])\n",
        "             .addGrid(rf.numTrees, [5, 20])\n",
        "             .build())\n",
        "\n",
        "# Se crea el CrossValidator (5-fold)\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# Se entrena el modelo (en función del número de combinaciones podría tardar)\n",
        "cvModel = cv.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wdJRsOBIuGk"
      },
      "source": [
        "Se evalua el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojpoVu0qIhDK",
        "outputId": "b0bff10e-7041-4846-9c05-b00b1cb9d99c"
      },
      "source": [
        "# Primero se predice sobre los datos de test\n",
        "# cvModel utiliza el mejor modelo que haya salido en la validación cruzada\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Y sobre las predicciones se puede evaluar el modelo\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8936864692496512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XSc9RBqsQgb"
      },
      "source": [
        "# Spark Stop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHm2F98TkLx"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}